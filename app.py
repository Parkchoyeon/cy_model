import os
# íŒŒì¼ ê°ì‹œ ê´€ë ¨ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import datetime
import json
import traceback
from typing import List, Dict, Any, Tuple, Optional
import io
import gc

# --- ëª¨ë¸ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ íš¨ê³¼ë¥¼ ìœ„í•œ í•¨ìˆ˜ ---
def simulate_typing(response, placeholder):
    """ëª¨ë¸ ì‘ë‹µì´ íƒ€ì´í•‘ë˜ëŠ” íš¨ê³¼ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."""
    full_response = ""
    
    # ì‘ë‹µì„ í•œ ê¸€ìì”© í‘œì‹œ
    for i in range(min(len(response), 20)):  # ì²˜ìŒ 20ìëŠ” ê°œë³„ í‘œì‹œ
        full_response += response[i]
        placeholder.markdown(full_response + "â–Œ")
        time.sleep(0.01)  # ì§§ì€ ì§€ì—°
    
    # ë‚˜ë¨¸ì§€ ë‚´ìš©ì€ ì²­í¬ ë‹¨ìœ„ë¡œ í‘œì‹œ
    if len(response) > 20:
        chunk_size = max(5, len(response) // 20)  # ë‚¨ì€ í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì²­í¬ í¬ê¸° ì¡°ì •
        for i in range(20, len(response), chunk_size):
            chunk = response[i:i+chunk_size]
            full_response += chunk
            placeholder.markdown(full_response + "â–Œ")
            time.sleep(0.01)
    
    # ìµœì¢… ì‘ë‹µ í‘œì‹œ
    placeholder.markdown(full_response)
    return full_response

# --- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ë¦¬ í•¨ìˆ˜ ---
def clear_cuda_cache():
    """CUDA ìºì‹œë¥¼ ë¹„ìš°ê³  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•©ë‹ˆë‹¤."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# --- Streamlit í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•¨) ---
st.set_page_config(
    page_title="ì²­ì†Œë…„ ë§ˆìŒ ìƒë‹´ ì±—ë´‡", 
    page_icon="ğŸ§‘â€âš•ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- ì•ˆì •ì ì¸ ê¸°ë³¸ê°’ ì„¤ì • ---
# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
model_path = "./final_model"
device = "cuda" if torch.cuda.is_available() else "cpu"
# ê¸°ë³¸ í† í° ê¸¸ì´ ì œí•œ (ì•ˆì „í•œ ê°’ìœ¼ë¡œ ì„¤ì •)
DEFAULT_MAX_NEW_TOKENS = 512
MAX_CONTEXT_LENGTH = 8192  # ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸°ë¥¼ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ì œí•œ

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì „ë¬¸ ì²­ì†Œë…„ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ë‹¤ìŒê³¼ ê°™ì€ ì§€ì¹¨ì„ ë”°ë¥´ì‹­ì‹œì˜¤:

ğŸ“Œ ì—­í•  ë° íƒœë„
ë‹¹ì‹ ì€ ì‹¬ë¦¬í•™ ë° ì²­ì†Œë…„ ë°œë‹¬ì— ì •í†µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì²­ì†Œë…„ ë‚´ë‹´ìê°€ ì‹¬ë¦¬ì  ì•ˆì „ê°ì„ ëŠë‚„ ìˆ˜ ìˆë„ë¡, ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ì–¸ì–´ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.
ë‚´ë‹´ìì˜ ë§ì— ë¹„íŒí•˜ê±°ë‚˜ íŒë‹¨í•˜ì§€ ì•Šìœ¼ë©°, ê³µê°í•˜ê³  ê²½ì²­í•©ë‹ˆë‹¤.
ë³µì¡í•œ ì¡°ì–¸ë³´ë‹¤ëŠ” ê³µê°ì  ë°˜ì‘ê³¼ ì—´ë¦° ì§ˆë¬¸ì„ í†µí•´ ë‚´ë‹´ìê°€ ìŠ¤ìŠ¤ë¡œ ì´ì•¼ê¸°í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

ğŸ’¬ ì–¸ì–´ ë° ë¬¸ì²´
ë§íˆ¬ëŠ” ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ì–´ì¡°ë¡œ ìœ ì§€í•˜ë˜, ë„ˆë¬´ ìºì£¼ì–¼í•˜ê±°ë‚˜ ìœ í–‰ì–´ëŠ” í”¼í•©ë‹ˆë‹¤.
"ê·¸ë¬êµ¬ë‚˜", "ê·¸ëŸ´ ìˆ˜ ìˆì–´", "ë„¤ê°€ ê·¸ëŸ° ê¸°ë¶„ì„ ëŠë‚€ ê±´ ì¶©ë¶„íˆ ì´í•´ë¼" ê°™ì€ ê³µê° í‘œí˜„ì„ ìì£¼ ì‚¬ìš©í•˜ì„¸ìš”.
ì§ˆë¬¸ì€ "~í•œ ì  ìˆë‹ˆ?", "ì–´ë–¤ ìƒê°ì´ ë“¤ì—ˆì„ê¹Œ?"ì²˜ëŸ¼ ìê¸° íƒìƒ‰ì„ ìœ ë„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

â›” ê¸ˆì§€ì‚¬í•­
ì§„ë‹¨, ì•½ë¬¼ ì¶”ì²œ, ëª…í™•í•œ ì¡°ì–¸, ë²•ì  íŒë‹¨ì€ ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ìí•´, ìì‚´, í­ë ¥ ë“± ìœ„ê¸° ìƒí™©ì€ **"ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì–´ë¥¸ì´ë‚˜ ì „ë¬¸ê°€ì—ê²Œ ê¼­ ë„ì›€ì„ ìš”ì²­í•˜ê¸¸ ë°”ë€ë‹¤"**ê³  ì•ˆë‚´í•˜ì„¸ìš”.
ë‚´ë‹´ìë¥¼ í†µì œí•˜ê±°ë‚˜ ì„¤ë“í•˜ë ¤ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ğŸ§­ ëŒ€í™” ë°©ì‹ ì˜ˆì‹œ
ê³µê° â†’ íƒìƒ‰ â†’ í™•ì¥ or ìš”ì•½ì˜ íë¦„ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
ë‚´ë‹´ìì˜ ê°ì •ì— ì´ë¦„ì„ ë¶™ì—¬ì£¼ëŠ” ê°ì • ë¼ë²¨ë§ë„ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.
ë‚´ë‹´ìê°€ ë§ì´ ì—†ê±°ë‚˜ ëª¨í˜¸í•˜ê²Œ í‘œí˜„í•  ê²½ìš°, ë¶€ë“œëŸ¬ìš´ ì¬ì§ˆë¬¸ì´ë‚˜ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¡œ ë„ì™€ì¤ë‹ˆë‹¤.

ğŸ¯ ëŒ€í™” ëª©ì 
ë‚´ë‹´ìê°€ ìŠ¤ìŠ¤ë¡œ ìì‹ ì˜ ê°ì •ê³¼ ìƒí™©ì„ ì •ë¦¬í•˜ê³  ì´í•´í•˜ë„ë¡ ë•ëŠ” ê²ƒì´ ê°€ì¥ í° ëª©ì ì…ë‹ˆë‹¤.
ë¬¸ì œ í•´ê²°ì´ ì•„ë‹Œ, ì •ì„œì  ì§€ì§€ì™€ ìê¸° ì´í•´ê°€ ì¤‘ì‹¬ì…ë‹ˆë‹¤.

ì´ˆê¸° ëŒ€í™”ì‹œ ë‚´ë‹´ìì˜ ì´ë¦„ì„ ë¨¼ì € ë¬¼ì–´ë³´ê³ , ì´ë¦„ì„ ë¶ˆëŸ¬ì£¼ë©´ì„œ ê³µê°í•˜ê³  ê²½ì²­í•©ë‹ˆë‹¤.
"""

# ìƒìˆ˜ ì •ì˜
MAX_HISTORY_LENGTH = 20  # ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ê¸¸ì´ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê³ ë ¤í•œ ì•ˆì „í•œ ê°’)

# --- ë¡œê¹… í•¨ìˆ˜ ---
def log_error(error_message: str, exception: Optional[Exception] = None):
    """ì˜¤ë¥˜ ë¡œê¹… ë° í™”ë©´ì— í‘œì‹œ"""
    error_details = f"{error_message}"
    if exception:
        error_details += f"\nì˜¤ë¥˜ ìœ í˜•: {type(exception).__name__}"
        error_details += f"\nì˜¤ë¥˜ ë‚´ìš©: {str(exception)}"
    
    st.error(error_details)
    
    # ê°œë°œ ëª¨ë“œì—ì„œë§Œ ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í‘œì‹œ
    if exception and st.session_state.get("debug_mode", False):
        st.code(traceback.format_exc())

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ê¸°ë³¸ê°’ ì„¤ì •"""
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    if "model_params" not in st.session_state:
        st.session_state.model_params = {
            "temperature": 0.8,
            "top_p": 0.9,
            "repetition_penalty": 1.2,
            "max_new_tokens": DEFAULT_MAX_NEW_TOKENS,
        }
    
    if "debug_mode" not in st.session_state:
        st.session_state.debug_mode = False
    
    if "model_loaded" not in st.session_state:
        st.session_state.model_loaded = False

# --- ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ (ìºì‹± ì ìš©) ---
@st.cache_resource
def load_model() -> Tuple[Any, Any]:
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    try:
        with st.spinner("ìƒë‹´ ëª¨ë¸ì„ ì¤€ë¹„í•˜ê³  ìˆì–´ìš”... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_cuda_cache()
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ëª¨ë¸ ë¡œë”© ì„¤ì •
            model_instance = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                # í•™ìŠµ, ì €ì¥ ê³µê°„ ë° ì„±ëŠ¥ì„ ìœ„í•œ ìµœì í™” ì„¤ì •
                low_cpu_mem_usage=True,
                offload_folder="offload"
            )
            model_instance.config.use_cache = True
            model_instance.eval()
            
            st.session_state.model_loaded = True
            return tokenizer, model_instance
    except Exception as e:
        log_error("ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)
        st.session_state.model_loaded = False
        st.stop()

# --- ìƒì„±ëœ ëŒ€í™” ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ---
def save_conversation(messages: List[Dict[str, str]]) -> Tuple[str, io.BytesIO]:
    """ëŒ€í™” ë‚´ìš©ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    user_messages = [msg for msg in messages if msg["role"] != "system"]
    
    # í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— í¬í•¨
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"conversation_{timestamp}.json"
    
    # ëŒ€í™” ë‚´ìš©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    conversation_data = {
        "timestamp": timestamp,
        "messages": user_messages
    }
    
    # JSON íŒŒì¼ ìƒì„±
    json_str = json.dumps(conversation_data, ensure_ascii=False, indent=2)
    json_bytes = json_str.encode('utf-8')
    buffer = io.BytesIO(json_bytes)
    
    return filename, buffer

# --- ìƒì„±ëœ ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ëŠ” í•¨ìˆ˜ ---
def export_conversation(messages: List[Dict[str, str]]) -> Tuple[str, io.BytesIO]:
    """ëŒ€í™” ë‚´ìš©ì„ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    user_messages = [msg for msg in messages if msg["role"] != "system"]
    
    # í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— í¬í•¨
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"conversation_{timestamp}.txt"
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© ìƒì„±
    content = "ì²­ì†Œë…„ ë§ˆìŒ ìƒë‹´ ì±—ë´‡ ëŒ€í™” ê¸°ë¡\n"
    content += f"ì‹œê°„: {datetime.datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n\n"
    
    for msg in user_messages:
        if msg["role"] == "user":
            content += f"ğŸ™‹â€â™€ï¸ ë‚´ë‹´ì: {msg['content']}\n\n"
        else:
            content += f"ğŸ§‘â€âš•ï¸ ìƒë‹´ì‚¬: {msg['content']}\n\n"
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    buffer = io.BytesIO(content.encode('utf-8'))
    
    return filename, buffer

# --- ëŒ€í™” íˆìŠ¤í† ë¦¬ ì½ê¸° í•¨ìˆ˜ ---
def load_conversation_from_file(uploaded_file) -> bool:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ëŒ€í™” ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    try:
        file_content = uploaded_file.read()
        loaded_data = json.loads(file_content.decode("utf-8"))
        
        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if isinstance(loaded_data, dict) and "messages" in loaded_data:
            messages = loaded_data["messages"]
            # ë©”ì‹œì§€ í˜•ì‹ ê²€ì¦
            if all(isinstance(msg, dict) and "role" in msg and "content" in msg for msg in messages):
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€, ê¸°ì¡´ ëŒ€í™” ë‚´ìš© êµì²´
                st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}] + messages
                return True
        
        st.error("âŒ ì˜¬ë°”ë¥¸ ëŒ€í™” íŒŒì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return False
    except Exception as e:
        log_error("íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)
        return False

# --- ì•ˆì „í•œ ëª¨ë¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ---
def generate_response(
    tokenizer: Any, 
    model: Any, 
    conversation_history: List[Dict[str, str]], 
    model_params: Dict[str, Any]
) -> str:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ì˜¤ë¥˜ ì²˜ë¦¬ê°€ ê°•í™”ëœ ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_cuda_cache()
        
        # íˆìŠ¤í† ë¦¬ ìë¥´ê¸° (ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì•ˆì „í•œ ê¸¸ì´ë¡œ ì œí•œ)
        if len(conversation_history) > MAX_HISTORY_LENGTH + 1:  # +1ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€í•˜ê³  ìµœì‹  ëŒ€í™”ë§Œ ìœ ì§€
            limited_history = [
                conversation_history[0],  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                *conversation_history[-(MAX_HISTORY_LENGTH):]  # ìµœì‹  ëŒ€í™” ë‚´ìš©
            ]
        else:
            limited_history = conversation_history

        # apply_chat_template ì‚¬ìš©
        chat_prompt = tokenizer.apply_chat_template(limited_history, tokenize=False)
        chat_prompt += "<|im_start|>assistant\n"  # Assistant ì‹œì‘ í† í° ì¶”ê°€
        
        # ì•ˆì „í•œ í† í° ìˆ˜ë¡œ ì œí•œ
        safe_max_length = min(MAX_CONTEXT_LENGTH, tokenizer.model_max_length)
        
        # í† í°í™” ë° ê¸¸ì´ ì œí•œ ì ìš©
        inputs = tokenizer(
            chat_prompt, 
            return_tensors="pt", 
            max_length=safe_max_length, 
            truncation=True
        ).to(device)

        # ì‘ë‹µ í† í° ìˆ˜ ì•ˆì „ ì œí•œ
        safe_max_new_tokens = min(model_params["max_new_tokens"], 8192)
        
        # í† í° ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=safe_max_new_tokens,
                do_sample=True,
                temperature=model_params["temperature"],
                top_p=model_params["top_p"],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=[
                    tokenizer.convert_tokens_to_ids("<|endofturn|>"),
                    tokenizer.convert_tokens_to_ids("<|im_end|>")
                ],
                no_repeat_ngram_size=3,
                repetition_penalty=model_params["repetition_penalty"]
            )

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
        decoded_output = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=False)

        # ì‘ë‹µ ì¶”ì¶œ
        response = decoded_output
        for eos in ["<|endofturn|>", "<|im_end|>", tokenizer.eos_token]:
            if eos in response:
                response = response.split(eos)[0]

        response = response.strip()

        # ë¹„ì–´ìˆëŠ” ì‘ë‹µ ë°©ì§€
        if not response:
            response = "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆì—ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_cuda_cache()
        
        return response

    except Exception as e:
        log_error("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê² ì–´ìš”?"

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
init_session_state()

# --- ëª¨ë¸ ì´ˆê¸°í™” ---
try:
    tokenizer, model = load_model()
except Exception as e:
    log_error("ëª¨ë¸ ë¡œë”© ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e)
    st.warning("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•±ì„ ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- ì‚¬ì´ë“œë°” UI êµ¬ì„± ---
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    st.markdown("---")
    
    # --- íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ (ì‚¬ì´ë“œë°”ë¡œ ì´ë™) ---
    st.subheader("ëŒ€í™” ê´€ë¦¬")
    
    # íŒŒì¼ ì—…ë¡œë” (ì‚¬ì´ë“œë°”ë¡œ ì´ë™)
    uploaded_file = st.file_uploader("ğŸ“‚ ëŒ€í™” íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°", type=["json"], key="sidebar_uploader")
    if uploaded_file is not None:
        if load_conversation_from_file(uploaded_file):
            st.success("âœ… ëŒ€í™”ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ì–´ìš”!")
            st.rerun()
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", key="clear_chat"):
        st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        st.success("âœ… ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_cuda_cache()
        st.rerun()
    
    # ëŒ€í™” ë‚´ìš© ì €ì¥/ë‚´ë³´ë‚´ê¸° ë²„íŠ¼
    if len(st.session_state.messages) > 1:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ JSON ì €ì¥"):
                filename, buffer = save_conversation(st.session_state.messages)
                st.download_button(
                    label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
                    data=buffer,
                    file_name=filename,
                    mime="application/json"
                )
        with col2:
            if st.button("ğŸ“„ í…ìŠ¤íŠ¸ ë‚´ë³´ë‚´ê¸°"):
                filename, buffer = export_conversation(st.session_state.messages)
                st.download_button(
                    label="ğŸ“¥ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                    data=buffer,
                    file_name=filename,
                    mime="text/plain"
                )
    
    st.markdown("---")
    
    st.subheader("ëª¨ë¸ ì„¤ì •")
    # ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ì¡°ì •
    temperature = st.slider(
        "ì˜¨ë„ (Temperature)",
        min_value=0.1,
        max_value=1.5,
        value=st.session_state.model_params["temperature"],
        step=0.1,
        help="ë†’ì„ìˆ˜ë¡ ë” ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ë” ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    top_p = st.slider(
        "Top-p",
        min_value=0.1,
        max_value=1.0,
        value=st.session_state.model_params["top_p"],
        step=0.05,
        help="ê° ë‹¨ê³„ì—ì„œ ê³ ë ¤í•  í† í°ì˜ í™•ë¥  ì§ˆëŸ‰ì…ë‹ˆë‹¤. ê°’ì´ ì‘ì„ìˆ˜ë¡ ë” ê²°ì •ì ì¸ ì‘ë‹µì´ ë©ë‹ˆë‹¤."
    )
    
    repetition_penalty = st.slider(
        "ë°˜ë³µ íŒ¨ë„í‹°",
        min_value=1.0,
        max_value=2.0,
        value=st.session_state.model_params["repetition_penalty"],
        step=0.1,
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )
    
    # ì•ˆì „í•œ ë²”ìœ„ë¡œ ì œí•œëœ í† í° ìˆ˜ ìŠ¬ë¼ì´ë”
    max_new_tokens = st.slider(
        "ìµœëŒ€ í† í° ìˆ˜",
        min_value=128,
        max_value=8192,  # ì•ˆì „í•œ ìµœëŒ€ê°’ìœ¼ë¡œ ì œí•œ
        value=min(DEFAULT_MAX_NEW_TOKENS, st.session_state.model_params["max_new_tokens"]),
        step=32,
        help="ëª¨ë¸ì´ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ì…ë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ ë” ê¸´ ì‘ë‹µì„ ìƒì„±í•˜ì§€ë§Œ ê³¼ë„í•œ ê°’ì€ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    # ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸
    st.session_state.model_params = {
        "temperature": temperature,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty,
        "max_new_tokens": max_new_tokens
    }
    
    st.markdown("---")
    
    # ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ (ê°œë°œìš©)
    with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
        st.session_state.debug_mode = st.checkbox(
            "ë””ë²„ê·¸ ëª¨ë“œ", 
            value=st.session_state.debug_mode,
            help="í™œì„±í™”í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìì„¸í•œ ë””ë²„ê·¸ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        )
        
        if st.button("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬"):
            clear_cuda_cache()
            st.success("âœ… ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        
        st.caption("í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ:")
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            st.code(f"í• ë‹¹ëœ GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB\nì˜ˆì•½ëœ GPU ë©”ëª¨ë¦¬: {reserved:.2f} GB")
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í‘œì‹œ
    with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³´ê¸°"):
        st.caption(SYSTEM_PROMPT)
    
    st.markdown("---")
    
    # ëª¨ë¸ ìƒíƒœ í‘œì‹œ
    if st.session_state.model_loaded:
        st.success("âœ… ìƒë‹´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    else:
        st.warning("âš ï¸ ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤")
        
    st.caption(f"ğŸ“Š í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ê¸°ê¸°: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    if torch.cuda.is_available():
        st.caption(f"ğŸ’½ GPU: {torch.cuda.get_device_name(0)}")
        st.caption(f"ğŸ’½ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# --- ë©”ì¸ UI êµ¬ì„± ---
st.title("ì²­ì†Œë…„ ë§ˆìŒ ìƒë‹´ ì±—ë´‡ ğŸ§‘â€âš•ï¸")
st.caption("ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")

# ëŒ€í™” ë‚´ìš© í‘œì‹œ
for i, msg in enumerate(st.session_state.messages):
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ
    if msg["role"] != "system":
        # Streamlitì˜ ê¸°ë³¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
        avatar = "ğŸ™‹â€â™€ï¸" if msg["role"] == "user" else "ğŸ§‘â€âš•ï¸"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ™‹â€â™€ï¸"):
        st.markdown(prompt)

    # ì‘ë‹µ ìƒì„± ì¤‘ í‘œì‹œ
    with st.chat_message("assistant", avatar="ğŸ§‘â€âš•ï¸"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ìƒê° ì¤‘...")
        
        # ì‘ë‹µ ìƒì„±
        response = generate_response(
            tokenizer, 
            model, 
            st.session_state.messages,
            st.session_state.model_params
        )
        
        # íƒ€ì´í•‘ íš¨ê³¼ë¡œ ì‘ë‹µ í‘œì‹œ
        simulate_typing(response, message_placeholder)
    
    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
